{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://openenergy-platform.org/static/OEP_logo_2_no_text.svg\" alt=\"OpenEnergy Platform\" height=\"100\" width=\"100\"  align=\"left\"/>\n",
    "\n",
    "# OpenEnergyPlatform\n",
    "<br><br>\n",
    "\n",
    "## Usage of OpenEnergyPlatform oem2orm tool via the API-Dialect (oedialect)\n",
    "Repository: https://github.com/openego/oedialect <br>\n",
    "Documentation: http://oep-data-interface.readthedocs.io/en/latest/api/how_to.html\n",
    "\n",
    "Please report bugs and improvements here: https://github.com/OpenEnergyPlatform/oedialect/issues <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__copyright__ = \"Reiner Lemoine Institut\"\n",
    "__license__   = \"GNU Affero General Public License Version 3 (AGPL-3.0)\"\n",
    "__url__       = \"https://github.com/openego/data_processing/blob/master/LICENSE\"\n",
    "__author__    = \"jh-RLI, christian-rli\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Creating sql tables, reading spatial-data, uploading to the oedb\n",
    "\n",
    "Takeaways:\n",
    "- How to create a table on the OEP from a oemetadata file\n",
    "- How to read sptial data (from .gkpg files) in python\n",
    "- How to upload this data to the OEP using the OEP-API and the oedialect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "First we set up the environment with all dependencys (see requirements.txt) and provide the credentials to connect to the OEP. Then we setup our sql tables, we will do this using the oemetadata format in v1.4.0. The metadata strings should be reviewed first in order to avoid unsupported datatypes or other inconsistencies inside the string. If we use our own oem data this can lead to errors in the next steps. We use the oem2orm package to create sqlalchemy tables that are derived from the oemetadata and then create the tables on the oep using the oep API (sqlachemy with oedialect). After that you should always check if the tables exist and are created properly. If this looks fine we can proceed to the next step and import our spatial data into a geopandas dataframe in python and then upload the data using the oedialect again. Geopandas provides all i/o functionality to do so. In this tutorial we focus on reading spatial-data from .gpkg files. \n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the oem2orm [usage](https://github.com/OpenEnergyPlatform/data-preprocessing/blob/feature/oep-upload-oem2orm/data-import/oep-upload/README.md) details, it's best practice to clone this [GitHub](https://github.com/OpenEnergyPlatform/data-preprocessing) repository, as we only want to upload data that has been properly reviewed. The reviewed data can be found in the  If you want, you can still use your own data with this example, but be sure to delete your tables afterwards. The oem2orm tool also requires the use of [Open Energy Metadata (oem)](https://github.com/OpenEnergyPlatform/metadata/blob/develop/metadata/v140/template.json)in v1.4.0 or lower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import getpass\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import oedialect\n",
    "from oem2orm import oep_oedialect_oem2orm as oem2orm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to OEP\n",
    "\n",
    "If we want to upload data to the OEP we first need to connect to it, using our OEP user name and token.\n",
    "\n",
    "Note: You can view your token on your OEP profile page after logging in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# White spaces in the username are fine!\n",
    "user = input('Enter OEP-username:')\n",
    "token = getpass.getpass('Token:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create an sql-alchemy-engine. The engine is what 'speaks' oedialect to the data base api. We need to tell it where the data base is and pass our credentials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Engine:\n",
    "OEP_URL = 'openenergy-platform.org' #'193.175.187.164' #'oep.iks.cs.ovgu.de'\n",
    "OED_STRING = f'postgresql+oedialect://{user}:{token}@{OEP_URL}'\n",
    "\n",
    "engine = sa.create_engine(OED_STRING)\n",
    "metadata = sa.MetaData(bind=engine)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to OEP (oem2orm)\n",
    "\n",
    "If we want to upload data to the OEP we first need to connect to it, using our OEP user name and token.\n",
    "\n",
    "Note: You can view your token on your OEP profile page after logging in. \n",
    "\n",
    "The setup_db_connection function will promt for the user credentials and returns the DB nametuple which \n",
    "is used for all database interactions. DB contains the sqlachemy engine and metadata object. We don't need \n",
    "to pass parameters to the function, because we use the OEP in this example, which is the default database \n",
    "for oem2orm functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = oem2orm.setup_db_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating sql tables from oemetadata\n",
    "\n",
    "The oemetadata format is a json file format which is required for all data which should be uploaded to the oep. An advantage is that the data model with the used data types is included. So it is possible to derive sqlalchemy tables from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide an oemetadata file / Data Input (oem2orm)\n",
    "\n",
    "In order to create the table we need to tell python where to find our oemetadata file first. To do this we place the oem (v1.4.0) file in the folder \"upload-example-metadata\" in the current directory (Path of this jupyter notebbok) or provide a path to our oemetadata folder. oem2orm is able to process all files that are located in a folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_folder = oem2orm.select_oem_dir(oem_folder_name=oem-upload-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a Table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'example_dialect_table'\n",
    "schema_name = 'sandbox'\n",
    "\n",
    "ExampleTable = sa.Table(\n",
    "    table_name,\n",
    "    metadata,\n",
    "    sa.Column('variable', sa.VARCHAR(50)),\n",
    "    sa.Column('unit', sa.VARCHAR(50)),\n",
    "    sa.Column('year', sa.INTEGER),\n",
    "    sa.Column('value', sa.FLOAT(50)),\n",
    "    schema=schema_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a Table (oem2orm)\n",
    "\n",
    "The collect_tables_function collects all metadata files in a folder and retrives the SQLAlchemy ORM objects and returns them.\n",
    "The Tables are ordered by forigen key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_orm = oem2orm.collect_ordered_tables_from_oem(metadata_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the new Table\n",
    "\n",
    "Now we tell our engine to connect to the data base and create the defined table within the chosen schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.connect()\n",
    "print('Connection established')\n",
    "if not engine.dialect.has_table(conn, table_name, schema_name):\n",
    "    ExampleTable.create()\n",
    "    print('Created table')\n",
    "else:\n",
    "    print('Table already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the new Table (oem2orm)\n",
    "\n",
    "Now we can use the function create_tables() from oem2orm to create all of our Table objects we just created in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oem2orm.create_tables(db, ordered_orm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading spatial-data and 3. uploading it to the oedb\n",
    "\n",
    "Geopandas offers functionality for spatial data. The read_file() function can read data from several sources \n",
    "e.g. .gkpg, .geojson, ... the function is also able to import the data by using an url that provides the data.\n",
    "\n",
    "FYI see: https://geopandas.org/io.html\n",
    "\n",
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not UnparsedPath",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bd22ac1d73f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexample_gdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/TemplateData.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\oep-upload\\lib\\site-packages\\geopandas\\io\\file.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\oep-upload\\lib\\site-packages\\fiona\\env.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\oep-upload\\lib\\site-packages\\fiona\\__init__.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[1;32m--> 254\u001b[1;33m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\oep-upload\\lib\\site-packages\\fiona\\collection.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\oep-upload\\lib\\ntpath.py\u001b[0m in \u001b[0;36msplitext\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgenericpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_splitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not UnparsedPath"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "example_gdf = gpd.read_file('../data/TemplateData.csv', layer='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first three lines of our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert (upload) data into Table\n",
    " \n",
    "Uploading the information from our DataFrame is now done with a single command. Uploading data in this way will always delete the content of the table and refill it with new values every time. If you change 'replace' to 'append', the data entries will be added to the preexisting ones. (Connecting and uploading may take a minute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "try: \n",
    "    example_df.to_sql(table_name, conn, schema_name, if_exists='replace')\n",
    "    print('Inserted to ' + table_name)\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    raise\n",
    "    print('Insert incomplete!')\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also insert data manually into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "try:\n",
    "    insert_statement = ExampleTable.insert().values(\n",
    "        [\n",
    "            dict(variable='fairy dust', unit='t', year=2020, value=200),\n",
    "            dict(variable='mana', unit='kg', year=1999, value=120),\n",
    "            dict(variable='the force', unit='l', year=1998, value=1100)\n",
    "        ]\n",
    "    )\n",
    "    session.execute(insert_statement)\n",
    "    session.commit()\n",
    "    print('Insert successful!')\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    raise\n",
    "    print('Insert incomplete!')\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retriving data, verify that the tables exist and data is uploaded successfully\n",
    "\n",
    "### Select from Table\n",
    "\n",
    "Now  we can query our table to see if the data arrived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "print(session.query(ExampleTable).all())\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Query Result in DataFrame\n",
    "We can write the results of the query back into a DataFrame, where it's easier to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "df = pd.DataFrame(session.query(ExampleTable).all())\n",
    "session.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show oedialect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oep-upload",
   "language": "python",
   "name": "oep-upload"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
